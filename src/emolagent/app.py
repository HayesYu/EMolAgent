"""
EMolAgent Streamlit ä¸»åº”ç”¨

æä¾›åŸºäº Web çš„ç”¨æˆ·ç•Œé¢ï¼Œé›†æˆ LangChain Agent è¿›è¡Œåˆ†å­è®¡ç®—å’ŒçŸ¥è¯†é—®ç­”ã€‚
"""

import os
import json
import time
import datetime
import re
import shutil
from dataclasses import dataclass
from typing import Any

import streamlit as st
import extra_streamlit_components as stx

from emolagent.utils.logger import logger
from emolagent.utils.paths import get_resource_path, get_project_root

from emolagent.database import db
from emolagent.core.tools import (
    search_molecule_in_db,
    build_and_optimize_cluster,
    run_dm_infer_pipeline,
    compress_directory,
)

from emolagent.knowledge import (
    search_knowledge,
    build_index,
    get_index_stats,
    LITERATURE_PATH,
)

from langchain.agents import create_agent
from langchain.tools import tool, ToolRuntime
from langchain.agents.structured_output import ToolStrategy
from langgraph.checkpoint.memory import InMemorySaver
from langchain_google_genai import ChatGoogleGenerativeAI

from emolagent.visualization import (
    create_structure_preview_html, 
    load_structure_from_db, 
    create_gaussian_view_style_viewer,
    create_orbital_viewer,
    find_orbital_files,
    find_li_deformation_files,
    create_li_deformation_viewer,
    create_analysis_visualization_html,
    find_esp_files,
    create_esp_viewer,
)
import streamlit.components.v1 as components


DEFAULT_MODEL_PATH = get_resource_path("models", "nnenv.ep154.pth")

ADMIN_USERS = ["hayes"]

WELCOME_MESSAGE = """æ‚¨å¥½ï¼æˆ‘æ˜¯ EMolAgentï¼Œæ‚¨çš„è®¡ç®—åŒ–å­¦ AI åŠ©æ‰‹ã€‚

æˆ‘å…·å¤‡ä¸¤å¤§æ ¸å¿ƒèƒ½åŠ›ï¼š

ğŸ”¬ **åˆ†å­å›¢ç°‡è®¡ç®—**
ä»æœ¬åœ°æ•°æ®åº“æ£€ç´¢åˆ†å­ã€æ„å»ºå¹¶ä¼˜åŒ–å›¢ç°‡ç»“æ„ï¼Œé¢„æµ‹ HOMO/LUMOã€å¶æçŸ©åŠé™ç”µåŠ¿ç­‰ç”µå­æ€§è´¨ã€‚
ç¤ºä¾‹ï¼šã€Œè¯·æ„å»ºä¸€ä¸ªåŒ…å« 1ä¸ªLiç¦»å­ã€3ä¸ªDMEåˆ†å­ å’Œ 1ä¸ªFSIé˜´ç¦»å­ çš„å›¢ç°‡ã€

ğŸ“š **æ–‡çŒ®çŸ¥è¯†é—®ç­”**
åŸºäºæ•°ç™¾ç¯‡ AI for Science å’Œç”µè§£æ¶²é¢†åŸŸæ–‡çŒ®ï¼Œå›ç­”ç›¸å…³å­¦æœ¯é—®é¢˜ã€‚
ç¤ºä¾‹ï¼šã€Œä»€ä¹ˆæ˜¯æº¶å‰‚åŒ–ç»“æ„ï¼ŸCIPå’ŒSSIPæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿã€ã€Œä»‹ç»ä¸€ä¸‹ GNN åœ¨åˆ†å­æ€§è´¨é¢„æµ‹ä¸­çš„åº”ç”¨ã€

è¯·å‘Šè¯‰æˆ‘æ‚¨çš„éœ€æ±‚ï¼Œæˆ‘å°†ä¸ºæ‚¨æä¾›å¸®åŠ©ï¼"""

CUSTOM_SYSTEM_PREFIX = """
ä½ æ˜¯ä¸€ä¸ªè®¡ç®—åŒ–å­¦ AI åŠ©æ‰‹ EMolAgentã€‚ä½ æœ‰ä¸¤å¤§æ ¸å¿ƒèƒ½åŠ›ï¼š

## èƒ½åŠ›ä¸€ï¼šåˆ†å­å›¢ç°‡è®¡ç®—
è¯·éµå¾ªä»¥ä¸‹å·¥ä½œæµæ¥å¤„ç†ç”¨æˆ·çš„åˆ†å­è®¡ç®—è¯·æ±‚ï¼š

### é‡è¦ï¼šè¯†åˆ«ç”¨æˆ·æ„å›¾
- **åªç”Ÿæˆç»“æ„**ï¼šå½“ç”¨æˆ·è¯´"ç”Ÿæˆä¸€ä¸ªç»“æ„"ã€"æ„å»ºä¸€ä¸ªå›¢ç°‡"ã€"åˆ›å»ºåˆ†å­ç»“æ„"ç­‰ï¼Œ**åªè°ƒç”¨** `Build_Structure_Only`ï¼Œä¸è¦æ‰§è¡Œç”µå­ç»“æ„åˆ†æ
- **ç”Ÿæˆå¹¶åˆ†æ**ï¼šå½“ç”¨æˆ·æ˜ç¡®è¯´"ç”Ÿæˆå¹¶åˆ†æ"ã€"è®¡ç®—ç”µå­ç»“æ„"ã€"é¢„æµ‹æ€§è´¨"ç­‰ï¼Œæ‰æ‰§è¡Œå®Œæ•´æµç¨‹ï¼ˆåŒ…æ‹¬ `Run_Inference_Pipeline`ï¼‰
- **å¯¹å·²æœ‰ç»“æ„åˆ†æ**ï¼šå½“ç”¨æˆ·è¯´"å¯¹ä¸Šé¢çš„ç»“æ„è¿›è¡Œåˆ†æ"ã€"åˆ†æåˆšæ‰ç”Ÿæˆçš„ç»“æ„"ç­‰ï¼Œä»å¯¹è¯å†å²ä¸­æ‰¾åˆ°ä¹‹å‰ç”Ÿæˆçš„ `optimized_db` è·¯å¾„ï¼Œç„¶åè°ƒç”¨ `Run_Inference_Pipeline`

### å·¥ä½œæµæ­¥éª¤ï¼š

1.  **è§£æéœ€æ±‚**ï¼šè¯†åˆ«ç”¨æˆ·æƒ³è¦çš„ä¸­å¿ƒç¦»å­ï¼ˆå¦‚ Liï¼‰ã€æº¶å‰‚ï¼ˆå¦‚ DMEï¼‰å’Œé˜´ç¦»å­ï¼ˆå¦‚ FSIï¼‰åŠå…¶æ•°é‡ã€‚

2.  **æ•°æ®åº“æ£€ç´¢ (Search_Molecule_DB)**ï¼š
    * **ä¼˜å…ˆæŸ¥åº“**ï¼šå¯¹äºæåˆ°çš„æ¯ä¸ªåˆ†å­ï¼ˆæº¶å‰‚æˆ–é˜´ç¦»å­ï¼‰ï¼Œ**å¿…é¡»**å…ˆè°ƒç”¨ `Search_Molecule_DB` å°è¯•åœ¨æœ¬åœ°åº“ä¸­æŸ¥æ‰¾ã€‚
    * *Solvent* æŸ¥ 'solvent' ç±»å‹ï¼Œ*Salt/Anion* æŸ¥ 'anion' ç±»å‹ã€‚
    * **ç¡®è®¤åé¦ˆ**ï¼šå¦‚æœæ‰¾åˆ°äº†ï¼ˆè¿”å›äº† `db_path`ï¼‰ï¼Œå‘Šè¯‰ç”¨æˆ·"å·²åœ¨åº“ä¸­æ‰¾åˆ° DME (æ„å‹å·²æ ¡å‡†)"ã€‚å¦‚æœæ²¡æ‰¾åˆ°ï¼Œåˆ™å‡†å¤‡ä½¿ç”¨ SMILESï¼ˆä½ éœ€è¦è‡ªå·±çŸ¥é“æˆ–è¯¢é—®ç”¨æˆ·ï¼‰ã€‚

3.  **å»ºæ¨¡ä¸ä¼˜åŒ–**ï¼š
    * **åªç”Ÿæˆç»“æ„æ—¶**ï¼šè°ƒç”¨ `Build_Structure_Only`ï¼Œè¿”å›ç»“æ„è·¯å¾„å’Œå¯è§†åŒ–é¢„è§ˆ
    * **å®Œæ•´åˆ†ææ—¶**ï¼šè°ƒç”¨ `Build_and_Optimize`ï¼ˆä¼šè‡ªåŠ¨è¿›è¡Œ UMA ç»“æ„ä¼˜åŒ–ï¼‰
    * æ„é€  JSON å‚æ•°ã€‚
    * å¦‚æœç¬¬2æ­¥æ‰¾åˆ°äº† DB è·¯å¾„ï¼Œå‚æ•°é‡Œç”¨ `{"name": "DME", "path": "...", "count": 3}`ã€‚
    * å¦‚æœæ²¡æ‰¾åˆ°ï¼Œç”¨ `{"smiles": "...", "count": 3}`ã€‚

4.  **ç”µå­ç»“æ„æ¨æ–­ (Run_Inference_Pipeline)**ï¼ˆä»…å½“ç”¨æˆ·éœ€è¦åˆ†ææ—¶ï¼‰ï¼š
    * ä½¿ç”¨ä¸Šä¸€æ­¥æˆ–å¯¹è¯å†å²ä¸­çš„ `optimized_db` è·¯å¾„
    * æ‰§è¡Œæ¨æ–­å¹¶åˆ†ææ€§è´¨ï¼ˆHOMO/LUMO/Dipoleç­‰ï¼‰

5.  **æœ€ç»ˆæŠ¥å‘Š**ï¼š
    * å¦‚æœåªæ˜¯ç”Ÿæˆç»“æ„ï¼šå±•ç¤º 3D é¢„è§ˆï¼Œå‘ŠçŸ¥ç”¨æˆ·å¯ä»¥åç»­è¿›è¡Œåˆ†æ
    * å¦‚æœæ‰§è¡Œäº†åˆ†æï¼šå±•ç¤ºç”µå­æ€§è´¨ï¼Œ**å¿…é¡»ä¿ç•™** `[[DOWNLOAD:...]]` é“¾æ¥

### è®°ä½ï¼š
- ç”¨æˆ·è¯´"ç”Ÿæˆç»“æ„"â‰  éœ€è¦ç”µå­ç»“æ„åˆ†æ
- ç”¨æˆ·è¯´"åˆ†æä¸Šé¢çš„ç»“æ„"æ—¶ï¼Œä»ä¹‹å‰çš„å¯¹è¯å†å²ä¸­æŸ¥æ‰¾ `optimized_db` è·¯å¾„
- ç¡®ä¿ç»“æ„è·¯å¾„è¢«æ­£ç¡®è®°å½•ï¼Œä»¥ä¾¿åç»­åˆ†æä½¿ç”¨

## èƒ½åŠ›äºŒï¼šæ–‡çŒ®çŸ¥è¯†é—®ç­” (Search_Knowledge_Base)
å½“ç”¨æˆ·è¯¢é—®ä»¥ä¸‹ç±»å‹çš„é—®é¢˜æ—¶ï¼Œä½¿ç”¨ `Search_Knowledge_Base` å·¥å…·ï¼š
- AI for Science ç›¸å…³æ¨¡å‹å’Œæ–¹æ³•ï¼ˆå¦‚ GNNã€Transformerã€æ‰©æ•£æ¨¡å‹ç­‰ï¼‰
- ç”µè§£æ¶²æ€§è´¨ã€æº¶å‰‚åŒ–ç»“æ„ã€ç¦»å­ä¼ è¾“æœºç†
- ç”µæ± ææ–™ã€é”‚ç¦»å­/é’ ç¦»å­ç”µæ± 
- åˆ†å­æ¨¡æ‹Ÿæ–¹æ³•ã€DFTè®¡ç®—ã€æœºå™¨å­¦ä¹ åŠ¿å‡½æ•°
- ä»»ä½•éœ€è¦æ–‡çŒ®æ”¯æ’‘çš„ç§‘å­¦æ¦‚å¿µè§£é‡Š

**çŸ¥è¯†é—®ç­”å·¥ä½œæµ**ï¼š
1. ç†è§£ç”¨æˆ·é—®é¢˜çš„æ ¸å¿ƒæ¦‚å¿µ
2. è°ƒç”¨ `Search_Knowledge_Base` æœç´¢ç›¸å…³æ–‡çŒ®
3. åŸºäºæ£€ç´¢åˆ°çš„å†…å®¹ï¼Œç»“åˆä½ çš„çŸ¥è¯†è¿›è¡Œç»¼åˆå›ç­”
4. **å¿…é¡»å¼•ç”¨æ¥æº**ï¼Œæ ¼å¼å¦‚ï¼šã€Œæ ¹æ®æ–‡çŒ® [xxx.pdf] ...ã€

ã€æ³¨æ„ã€‘
* å¦‚æœç”¨æˆ·è¯´"3ä¸ªDME"ï¼Œæ„æ€æ˜¯ count=3ã€‚
* FSI é€šå¸¸æ˜¯é˜´ç¦»å­ã€‚
* ä¸€æ­¥æ­¥æ‰§è¡Œï¼Œä¸è¦è·³è¿‡"æŸ¥åº“"æ­¥éª¤ï¼Œå› ä¸ºåº“å†…æ„å‹è´¨é‡æœ€é«˜ã€‚
* å¯¹äºçŸ¥è¯†æ€§é—®é¢˜ï¼Œä¼˜å…ˆä½¿ç”¨çŸ¥è¯†åº“æœç´¢ï¼Œç¡®ä¿å›ç­”æœ‰æ–‡çŒ®ä¾æ®ã€‚
* **å…³é”®**ï¼šå½“ç”¨æˆ·åç»­è¯´"å¯¹ä¸Šé¢ç”Ÿæˆçš„ç»“æ„è¿›è¡Œåˆ†æ"æ—¶ï¼Œè¯·ä»ä¹‹å‰çš„å¯¹è¯ä¸­æ‰¾åˆ° optimized_db çš„è·¯å¾„ï¼Œå¹¶è°ƒç”¨ Run_Inference_Pipelineã€‚
"""

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="EMolAgent", page_icon="ğŸ§ª", layout="wide")


# ==============================================================================
# 1. è¾…åŠ©å‡½æ•°å®šä¹‰
# ==============================================================================

@st.cache_resource(ttl=86400)
def schedule_cleanup():
    """å®šæ—¶æ¸…ç†ä»»åŠ¡ã€‚"""
    db.cleanup_old_data(days=30)
    return True


def get_manager():
    return stx.CookieManager(key="auth_cookie_manager")


def validate_path_exists(path: str, description: str):
    """æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™ç»ˆæ­¢ã€‚"""
    if not path or not os.path.exists(path):
        st.error(f"â›”ï¸ **é”™è¯¯ï¼šç»ˆæ­¢æ‰§è¡Œ**\n\næ‰¾ä¸åˆ°{description}ï¼š`{path}`\n\nè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚")
        st.stop()
    return True


def get_user_workspace():
    """æ ¹æ® session_state ä¸­çš„ç”¨æˆ·ä¿¡æ¯å’Œå½“å‰ä¼šè¯IDç”Ÿæˆè·¯å¾„ã€‚"""
    if "user" in st.session_state and st.session_state["user"]:
        username = st.session_state["user"]["username"]
        safe_username = "".join([c for c in username if c.isalnum() or c in ("-", "_")])
        chat_id = st.session_state.get("current_chat_id", "temp")
        workspace = os.path.join(get_project_root(), "users", safe_username, "output", str(chat_id))
    else:
        workspace = os.path.join(get_project_root(), "users", "guest", "output", "temp")

    if not os.path.exists(workspace):
        os.makedirs(workspace, exist_ok=True)
    return workspace


def get_user_workspace_from_ids(username: str | None, chat_id: str | None):
    safe_username = "".join([c for c in (username or "guest") if c.isalnum() or c in ("-", "_")])
    safe_chat_id = str(chat_id or "temp")
    workspace = os.path.join(get_project_root(), "users", safe_username, "output", safe_chat_id)
    os.makedirs(workspace, exist_ok=True)
    return workspace


# ==============================================================================
# 2. Tools
# ==============================================================================

@dataclass
class Context:
    """è‡ªå®šä¹‰è¿è¡Œæ—¶ä¸Šä¸‹æ–‡ã€‚"""
    user_id: str | None = None
    username: str | None = None
    chat_id: str | None = None


@tool(
    "Search_Molecule_DB",
    description=(
        "Search for a molecule (solvent or anion) in the local calibrated database. "
        "Args: query_name (e.g., 'DME'), mol_type ('solvent' or 'anion'). "
        "Returns a string that includes db_path if found."
    ),
)
def tool_search_db(query_name: str, mol_type: str, runtime: ToolRuntime[Context]) -> str:
    """åœ¨æœ¬åœ°æ•°æ®åº“ä¸­æœç´¢åˆ†å­ã€‚"""
    user_ws = get_user_workspace_from_ids(runtime.context.username, runtime.context.chat_id)
    search_dir = os.path.join(user_ws, "search_cache")
    return search_molecule_in_db(query_name, mol_type, search_dir)


@tool(
    "Build_and_Optimize",
    description=(
        "Build a cluster and optimize it using UMA. "
        "Args: ion_name (str), solvents_json (JSON list), anions_json (JSON list). "
        "Each list item should have 'count' and either 'path' or 'smiles'."
    ),
)
def tool_build_optimize(ion_name: str, solvents_json: str, anions_json: str, runtime: ToolRuntime[Context]) -> str:
    """æ„å»ºå¹¶ä¼˜åŒ–å›¢ç°‡ã€‚"""
    try:
        solvents = json.loads(solvents_json) if solvents_json else []
        anions = json.loads(anions_json) if anions_json else []
    except Exception:
        return "Error parsing JSON inputs."

    user_ws = get_user_workspace_from_ids(runtime.context.username, runtime.context.chat_id)
    task_id = f"{time.strftime('%Y%m%d_%H%M%S')}_{time.time_ns()}"
    task_dir = os.path.join(user_ws, f"task_{task_id}")
    return build_and_optimize_cluster(ion_name, solvents, anions, task_dir)


@tool(
    "Build_Structure_Only",
    description=(
        "Build and optimize a molecular cluster structure WITHOUT running electronic structure analysis. "
        "Use this when user just wants to generate/build a structure. "
        "Args: ion_name (str), solvents_json (JSON list), anions_json (JSON list). "
        "Returns the optimized structure path and a 3D visualization for user confirmation."
    ),
)
def tool_build_structure_only(ion_name: str, solvents_json: str, anions_json: str, runtime: ToolRuntime[Context]) -> str:
    """ä»…æ„å»ºç»“æ„ï¼Œä¸è¿›è¡Œç”µå­ç»“æ„åˆ†æã€‚"""
    try:
        solvents = json.loads(solvents_json) if solvents_json else []
        anions = json.loads(anions_json) if anions_json else []
    except Exception:
        return "Error parsing JSON inputs."

    user_ws = get_user_workspace_from_ids(runtime.context.username, runtime.context.chat_id)
    task_id = f"{time.strftime('%Y%m%d_%H%M%S')}_{time.time_ns()}"
    task_dir = os.path.join(user_ws, f"task_{task_id}")
    
    result = build_and_optimize_cluster(ion_name, solvents, anions, task_dir)
    
    try:
        res_dict = json.loads(result)
        if res_dict.get("success"):
            optimized_db = res_dict.get("optimized_db")
            return json.dumps({
                "success": True,
                "optimized_db": optimized_db,
                "task_dir": task_dir,
                "msg": f"ç»“æ„å·²ç”Ÿæˆå¹¶ä¼˜åŒ–å®Œæˆã€‚è·¯å¾„: {optimized_db}",
                "visualization_marker": f"[[STRUCTURE_PREVIEW:{optimized_db}]]"
            })
        return result
    except:
        return result


@tool(
    "Run_Inference_Pipeline",
    description=(
        "Run DPTB inference and electronic structure analysis on optimized DB. "
        "Args: optimized_db_path (str), model_path (optional). "
        "Returns a string containing [[DOWNLOAD:...]] zip link on success."
    ),
)
def tool_infer_pipeline(optimized_db_path: str, model_path: str | None = None) -> str:
    """è¿è¡Œç”µå­ç»“æ„æ¨æ–­ã€‚"""
    if model_path in ["None", "", None]:
        model_path = DEFAULT_MODEL_PATH

    validate_path_exists(optimized_db_path, "Optimized DB")

    db_dir = os.path.dirname(optimized_db_path)
    parent_dir = os.path.dirname(db_dir)

    if os.path.basename(db_dir) == "final_optimized":
        task_root = parent_dir
    elif os.path.basename(db_dir).startswith("task_"):
        task_root = db_dir
    else:
        task_root = db_dir

    run_id = str(time.time_ns())
    infer_out = os.path.join(task_root, f"inference_results_{run_id}")
    result_json_str = run_dm_infer_pipeline(optimized_db_path, model_path, infer_out)

    try:
        res_dict = json.loads(result_json_str)
        if res_dict.get("success"):
            csv_path = res_dict.get("csv_path")
            output_dir = res_dict.get("output_dir", infer_out)
            zip_base_name = os.path.join(task_root, f"analysis_package_{run_id}")
            zip_path = compress_directory(output_dir, zip_base_name)

            return (
                f"æ¨ç†å®Œæˆã€‚\n"
                f"CSVæ‘˜è¦è·¯å¾„: {csv_path}\n"
                f"æ•°æ®é¢„è§ˆ: {res_dict.get('data_preview')}\n"
                f"[[ANALYSIS_VISUALIZATION:{optimized_db_path}|{infer_out}]]\n"
                f"[[DOWNLOAD:{zip_path}]]"
            )
        return f"æ¨ç†å‡ºé”™: {result_json_str}"
    except Exception as e:
        return f"Error processing inference results: {e}"


@tool(
    "Search_Knowledge_Base",
    description=(
        "Search the literature knowledge base for AI4Science and electrolyte-related content. "
        "Use this tool when user asks about: AI models, machine learning methods, electrolyte properties, "
        "battery materials, molecular simulation theories, or any scientific concepts. "
        "Args: query (str) - the search query in natural language, top_k (int, optional) - number of results. "
        "Returns relevant excerpts from academic papers with source citations."
    ),
)
def tool_search_knowledge(query: str, top_k: int = 5) -> str:
    """æœç´¢çŸ¥è¯†åº“ã€‚"""
    api_key = os.getenv("GOOGLE_API_KEY", "")
    if not api_key:
        return "Error: Google API Key not configured."
    
    try:
        results = search_knowledge(query, api_key, top_k=top_k)
        
        if not results:
            return "æœªæ‰¾åˆ°ç›¸å…³æ–‡çŒ®å†…å®¹ã€‚è¯·å°è¯•æ¢ä¸€ç§è¡¨è¾¾æ–¹å¼æˆ–æ›´å…·ä½“çš„å…³é”®è¯ã€‚"
        
        output_parts = [f"æ‰¾åˆ° {len(results)} æ¡ç›¸å…³æ–‡çŒ®å†…å®¹ï¼š\n"]
        
        for i, r in enumerate(results, 1):
            output_parts.append(
                f"**[{i}] {r['source']}** (ç›¸å…³åº¦: {r['relevance_score']:.2f})\n"
                f"åˆ†ç±»: {r['category'] or 'æ ¹ç›®å½•'}\n"
                f"å†…å®¹æ‘˜è¦:\n> {r['content'][:500]}{'...' if len(r['content']) > 500 else ''}\n"
            )
        
        return "\n---\n".join(output_parts)
    
    except Exception as e:
        return f"çŸ¥è¯†åº“æœç´¢å‡ºé”™: {str(e)}"


TOOLS = [tool_search_db, tool_build_structure_only, tool_build_optimize, tool_infer_pipeline, tool_search_knowledge]


# ==============================================================================
# 3. Agent åˆå§‹åŒ–
# ==============================================================================

@dataclass
class ResponseFormat:
    """ç»“æ„åŒ–å“åº”æ¨¡å¼ã€‚"""
    output: str


@st.cache_resource(show_spinner=False)
def get_checkpointer() -> InMemorySaver:
    return InMemorySaver()


def build_agent(model_name: str, temperature: float, api_key: str):
    """æ„å»ºå¹¶è¿”å› LangChain agentã€‚"""
    model = ChatGoogleGenerativeAI(
        model=model_name,
        google_api_key=api_key,
        temperature=temperature,
        timeout=30,
        max_output_tokens=2000,
    )

    checkpointer = get_checkpointer()

    agent = create_agent(
        model=model,
        tools=TOOLS,
        system_prompt=CUSTOM_SYSTEM_PREFIX,
        context_schema=Context,
        checkpointer=checkpointer,
    )
    return agent


# ==============================================================================
# 4. UIï¼šç™»å½• / ä¸»ç•Œé¢
# ==============================================================================

def login_ui(cookie_manager):
    """å¤„ç†ç™»å½•å’Œæ³¨å†Œçš„ UI æ¸²æŸ“ã€‚"""
    st.title("ğŸ§ª EMolAgent - è¯·å…ˆç™»å½•")
    tab1, tab2 = st.tabs(["ç™»å½•", "æ³¨å†Œ"])

    with tab1:
        with st.form("login_form"):
            username = st.text_input("ç”¨æˆ·å")
            password = st.text_input("å¯†ç ", type="password")
            submitted = st.form_submit_button("ç™»å½•")
            if submitted:
                user = db.login_user(username, password)
                if user:
                    st.session_state["user"] = user
                    st.session_state["current_chat_id"] = None
                    st.session_state["logout_flag"] = False
                    token = db.create_jwt_token(user["id"], user["username"])
                    expires = datetime.datetime.now() + datetime.timedelta(days=3)
                    cookie_manager.set("auth_token", token, expires_at=expires)
                    st.success("ç™»å½•æˆåŠŸï¼")
                    time.sleep(0.5)
                    st.rerun()
                else:
                    st.error("ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯")

    with tab2:
        with st.form("register_form"):
            new_user = st.text_input("æ–°ç”¨æˆ·å")
            new_pass = st.text_input("æ–°å¯†ç ", type="password")
            confirm_pass = st.text_input("ç¡®è®¤å¯†ç ", type="password")
            submitted = st.form_submit_button("æ³¨å†Œ")
            if submitted:
                if new_user and new_pass and confirm_pass:
                    if new_pass != confirm_pass:
                        st.error("ä¸¤æ¬¡è¾“å…¥çš„å¯†ç ä¸ä¸€è‡´")
                    elif db.register_user(new_user, new_pass):
                        st.success("æ³¨å†ŒæˆåŠŸï¼è¯·åˆ‡æ¢åˆ°ç™»å½•æ ‡ç­¾é¡µè¿›è¡Œç™»å½•ã€‚")
                    else:
                        st.error("ç”¨æˆ·åå·²å­˜åœ¨")
                else:
                    st.error("è¯·è¾“å…¥ç”¨æˆ·åå’Œå¯†ç ")


def normalize_chat_content(content: Any) -> str:
    if content is None:
        return ""
    if isinstance(content, str):
        return content
    if isinstance(content, bytes):
        try:
            return content.decode("utf-8", errors="replace")
        except Exception:
            return str(content)

    if isinstance(content, list):
        parts: list[str] = []
        for item in content:
            if item is None:
                continue
            if isinstance(item, str):
                parts.append(item)
            elif isinstance(item, dict):
                text = item.get("text")
                if isinstance(text, str):
                    parts.append(text)
                else:
                    parts.append(json.dumps(item, ensure_ascii=False, default=str))
            else:
                parts.append(str(item))
        return "\n".join([p for p in parts if p]).strip()

    if isinstance(content, dict):
        text = content.get("text")
        if isinstance(text, str):
            return text
        return json.dumps(content, ensure_ascii=False, default=str)

    return str(content)


def render_message_with_download(role: str, content: Any, key_prefix: str):
    """å°†ç‰¹æ®Šæ ‡è®°æ¸²æŸ“ä¸ºå¯äº¤äº’ç»„ä»¶ã€‚"""
    text = normalize_chat_content(content)

    with st.chat_message(role):
        structure_match = re.search(r"\[\[STRUCTURE_PREVIEW:(.*?)\]\]", text)
        analysis_match = re.search(r"\[\[ANALYSIS_VISUALIZATION:(.*?)\|(.*?)\]\]", text)
        download_match = re.search(r"\[\[DOWNLOAD:(.*?)\]\]", text)
        
        display_text = re.sub(r"\[\[STRUCTURE_PREVIEW:.*?\]\]", "", text)
        display_text = re.sub(r"\[\[ANALYSIS_VISUALIZATION:.*?\]\]", "", display_text)
        display_text = re.sub(r"\[\[DOWNLOAD:.*?\]\]", "", display_text).strip()
        st.write(display_text)

        if analysis_match:
            db_path = analysis_match.group(1).strip()
            infer_dir = analysis_match.group(2).strip()
            
            st.markdown("### ğŸ”¬ åˆ†æç»“æœå¯è§†åŒ–")
            
            # æŸ¥æ‰¾ Li deformation æ–‡ä»¶å’Œ ESP æ–‡ä»¶
            li_deform_files = find_li_deformation_files(infer_dir)
            esp_files = find_esp_files(infer_dir)
            has_esp = esp_files.get('density') and esp_files.get('esp')
            
            # æ ¹æ®å¯ç”¨æ–‡ä»¶å†³å®š tab æ•°é‡
            tab_names = ["ğŸ§¬ å›¢ç°‡ç»“æ„", "ğŸ”µ HOMO è½¨é“", "ğŸŸ¢ LUMO è½¨é“"]
            if has_esp:
                tab_names.append("âš¡ é™ç”µåŠ¿ (ESP)")
            if li_deform_files:
                tab_names.append("ğŸ’  Li Deformation")
            
            tabs = st.tabs(tab_names)
            tab_idx = 0
            tab_structure = tabs[tab_idx]; tab_idx += 1
            tab_homo = tabs[tab_idx]; tab_idx += 1
            tab_lumo = tabs[tab_idx]; tab_idx += 1
            tab_esp = tabs[tab_idx] if has_esp else None; tab_idx += (1 if has_esp else 0)
            tab_li_deform = tabs[tab_idx] if li_deform_files else None
            
            with tab_structure:
                if os.path.exists(db_path):
                    try:
                        atoms = load_structure_from_db(db_path)
                        if atoms:
                            viewer_html = create_gaussian_view_style_viewer(
                                atoms,
                                width=650,
                                height=500,
                                style="sphere+stick",
                                add_lighting=True
                            )
                            components.html(viewer_html, height=560, scrolling=False)
                            st.caption(f"åŒ–å­¦å¼: {atoms.get_chemical_formula()} | åŸå­æ•°: {len(atoms)}")
                        else:
                            st.warning("æ— æ³•åŠ è½½ç»“æ„é¢„è§ˆ")
                    except Exception as e:
                        st.error(f"ç»“æ„é¢„è§ˆå¤±è´¥: {e}")
                else:
                    st.warning(f"ç»“æ„æ–‡ä»¶ä¸å­˜åœ¨: {db_path}")
            
            orbital_files = find_orbital_files(infer_dir)
            
            with tab_homo:
                if orbital_files.get('homo') and os.path.exists(orbital_files['homo']):
                    try:
                        st.markdown("**ç­‰å€¼é¢è®¾ç½®**")
                        col1, col2 = st.columns([3, 1])
                        with col1:
                            homo_iso = st.slider(
                                "ç­‰å€¼é¢å¤§å°",
                                min_value=0.005,
                                max_value=0.1,
                                value=0.02,
                                step=0.005,
                                format="%.3f",
                                key=f"{key_prefix}_homo_iso",
                                help="è°ƒå¤§ï¼šè½¨é“åŒ…ç»œé¢æ”¶ç¼©ï¼›è°ƒå°ï¼šè½¨é“åŒ…ç»œé¢æ‰©å±•"
                            )
                        with col2:
                            st.metric("å½“å‰å€¼", f"{homo_iso:.3f}")
                        
                        homo_html = create_orbital_viewer(
                            orbital_files['homo'],
                            width=650,
                            height=500,
                            iso_value=homo_iso,
                            orbital_type="HOMO"
                        )
                        components.html(homo_html, height=560, scrolling=False)
                        st.caption(f"æ–‡ä»¶: {os.path.basename(orbital_files['homo'])}")
                    except Exception as e:
                        st.error(f"HOMO å¯è§†åŒ–å¤±è´¥: {e}")
                else:
                    st.info("HOMO è½¨é“æ–‡ä»¶æœªç”Ÿæˆæˆ–ä¸å¯ç”¨")
            
            with tab_lumo:
                if orbital_files.get('lumo') and os.path.exists(orbital_files['lumo']):
                    try:
                        st.markdown("**ç­‰å€¼é¢è®¾ç½®**")
                        col1, col2 = st.columns([3, 1])
                        with col1:
                            lumo_iso = st.slider(
                                "ç­‰å€¼é¢å¤§å°",
                                min_value=0.005,
                                max_value=0.1,
                                value=0.02,
                                step=0.005,
                                format="%.3f",
                                key=f"{key_prefix}_lumo_iso",
                                help="è°ƒå¤§ï¼šè½¨é“åŒ…ç»œé¢æ”¶ç¼©ï¼›è°ƒå°ï¼šè½¨é“åŒ…ç»œé¢æ‰©å±•"
                            )
                        with col2:
                            st.metric("å½“å‰å€¼", f"{lumo_iso:.3f}")
                        
                        lumo_html = create_orbital_viewer(
                            orbital_files['lumo'],
                            width=650,
                            height=500,
                            iso_value=lumo_iso,
                            orbital_type="LUMO"
                        )
                        components.html(lumo_html, height=560, scrolling=False)
                        st.caption(f"æ–‡ä»¶: {os.path.basename(orbital_files['lumo'])}")
                    except Exception as e:
                        st.error(f"LUMO å¯è§†åŒ–å¤±è´¥: {e}")
                else:
                    st.info("LUMO è½¨é“æ–‡ä»¶æœªç”Ÿæˆæˆ–ä¸å¯ç”¨")
            
            # ESP (é™ç”µåŠ¿) Tab
            if tab_esp is not None and has_esp:
                with tab_esp:
                    try:
                        st.markdown("**é™ç”µåŠ¿ (ESP) å¯è§†åŒ–**")
                        st.caption("å±•ç¤ºåˆ†å­è¡¨é¢é™ç”µåŠ¿åˆ†å¸ƒï¼šçº¢è‰²ä¸ºæ­£ï¼ˆäº²æ ¸åŒºåŸŸï¼‰ï¼Œè“è‰²ä¸ºè´Ÿï¼ˆäº²ç”µåŒºåŸŸï¼‰")
                        
                        # è‰²é˜¶èŒƒå›´æ§åˆ¶ (eV)
                        col1, col2 = st.columns([3, 1])
                        with col1:
                            esp_range_ev = st.slider(
                                "è‰²é˜¶èŒƒå›´ (eV)",
                                min_value=0.2,
                                max_value=3.0,
                                value=0.82,  # é»˜è®¤ 0.03 a.u. â‰ˆ 0.82 eV
                                step=0.1,
                                format="%.2f",
                                key=f"{key_prefix}_esp_range",
                                help="è°ƒæ•´ ESP è‰²é˜¶çš„æ˜¾ç¤ºèŒƒå›´ï¼Œè¶…å‡ºèŒƒå›´çš„å€¼ä¼šè¢«æˆªæ–­åˆ°è¾¹ç•Œé¢œè‰²"
                            )
                        with col2:
                            st.metric("Â±èŒƒå›´", f"{esp_range_ev:.2f} eV")
                        
                        # è½¬æ¢ä¸ºåŸå­å•ä½ (a.u.)
                        HARTREE_TO_EV = 27.2114
                        esp_colorscale_max = esp_range_ev / HARTREE_TO_EV
                        
                        esp_html = create_esp_viewer(
                            esp_files['density'],
                            esp_files['esp'],
                            esp_files.get('info'),
                            width=650,
                            height=500,
                            esp_colorscale_min=-esp_colorscale_max,
                            esp_colorscale_max=esp_colorscale_max,
                        )
                        components.html(esp_html, height=600, scrolling=False)
                        
                        # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
                        st.caption(f"å¯†åº¦æ–‡ä»¶: {os.path.basename(esp_files['density'])}")
                        st.caption(f"ESPæ–‡ä»¶: {os.path.basename(esp_files['esp'])}")
                        
                        # å¦‚æœæœ‰ ESP infoï¼Œæ˜¾ç¤ºæå€¼ä¿¡æ¯
                        if esp_files.get('info') and os.path.exists(esp_files['info']):
                            try:
                                import json as json_module
                                with open(esp_files['info'], 'r') as f:
                                    esp_info = json_module.load(f)
                                
                                st.markdown("---")
                                st.markdown("**ESP æå€¼ä¿¡æ¯**")
                                col_max, col_min = st.columns(2)
                                with col_max:
                                    max_val = esp_info.get('ESP_max_eV', 'N/A')
                                    max_loc = esp_info.get('ESP_max_location_Ang', [])
                                    st.metric("æœ€å¤§å€¼ (eV)", f"{max_val:.4f}" if isinstance(max_val, (int, float)) else max_val)
                                    if max_loc:
                                        st.caption(f"ä½ç½®: ({max_loc[0]:.2f}, {max_loc[1]:.2f}, {max_loc[2]:.2f}) Ã…")
                                with col_min:
                                    min_val = esp_info.get('ESP_min_eV', 'N/A')
                                    min_loc = esp_info.get('ESP_min_location_Ang', [])
                                    st.metric("æœ€å°å€¼ (eV)", f"{min_val:.4f}" if isinstance(min_val, (int, float)) else min_val)
                                    if min_loc:
                                        st.caption(f"ä½ç½®: ({min_loc[0]:.2f}, {min_loc[1]:.2f}, {min_loc[2]:.2f}) Ã…")
                            except Exception:
                                pass
                                
                    except Exception as e:
                        st.error(f"ESP å¯è§†åŒ–å¤±è´¥: {e}")
            
            # Li Deformation Tab
            if tab_li_deform is not None and li_deform_files:
                with tab_li_deform:
                    try:
                        st.markdown("**Li ç¦»å­å˜å½¢å› å­å¯è§†åŒ–**")
                        st.caption("å±•ç¤º Li ç¦»å­å‘¨å›´ç”µå­å¯†åº¦å˜å½¢çš„ç­‰å€¼é¢åˆ†å¸ƒ")
                        
                        # æŸ¥æ‰¾å¯¹åº”çš„åˆ†å­ç»“æ„ xyz æ–‡ä»¶
                        # ä¼˜å…ˆä» task ç›®å½•çš„ xyz_all ä¸­æŸ¥æ‰¾
                        task_dir = os.path.dirname(os.path.dirname(infer_dir))
                        xyz_all_dir = os.path.join(task_dir, "xyz_all")
                        molecule_path = None
                        
                        if os.path.exists(xyz_all_dir):
                            import glob as glob_module
                            xyz_files = glob_module.glob(os.path.join(xyz_all_dir, "*.xyz"))
                            if xyz_files:
                                molecule_path = xyz_files[0]  # å–ç¬¬ä¸€ä¸ª
                        
                        # å¦‚æœæ‰¾ä¸åˆ° xyzï¼Œå°è¯•ä» db å¯¼å‡º
                        if molecule_path is None and os.path.exists(db_path):
                            from emolagent.visualization import atoms_to_xyz_string
                            atoms = load_structure_from_db(db_path)
                            if atoms:
                                # åˆ›å»ºä¸´æ—¶ xyz æ–‡ä»¶
                                temp_xyz_path = os.path.join(infer_dir, "temp_molecule.xyz")
                                with open(temp_xyz_path, 'w') as f:
                                    f.write(atoms_to_xyz_string(atoms, "Generated for Li Deformation visualization"))
                                molecule_path = temp_xyz_path
                        
                        if molecule_path is None:
                            st.warning("æœªæ‰¾åˆ°åˆ†å­ç»“æ„æ–‡ä»¶ï¼Œæ— æ³•å åŠ æ˜¾ç¤º")
                        else:
                            # é€æ˜åº¦æ§åˆ¶
                            col1, col2 = st.columns([3, 1])
                            with col1:
                                opacity = st.slider(
                                    "è¡¨é¢é€æ˜åº¦",
                                    min_value=0.1,
                                    max_value=1.0,
                                    value=0.65,
                                    step=0.05,
                                    format="%.2f",
                                    key=f"{key_prefix}_li_deform_opacity",
                                    help="è°ƒæ•´ Li deformation è¡¨é¢çš„é€æ˜åº¦"
                                )
                            with col2:
                                st.metric("é€æ˜åº¦", f"{opacity:.2f}")
                            
                            # æ˜¾ç¤ºç¬¬ä¸€ä¸ª Li deformation æ–‡ä»¶
                            li_file = li_deform_files[0]
                            
                            li_deform_html = create_li_deformation_viewer(
                                molecule_path=molecule_path,
                                surface_pdb_path=li_file['path'],
                                width=650,
                                height=500,
                                surface_opacity=opacity,
                                isovalue=li_file.get('isovalue', '0.09'),
                            )
                            components.html(li_deform_html, height=560, scrolling=False)
                            st.caption(f"æ–‡ä»¶: {os.path.basename(li_file['path'])} | ç­‰å€¼é¢: {li_file.get('isovalue', 'N/A')}")
                            
                            # å¦‚æœæœ‰å¤šä¸ªæ–‡ä»¶ï¼Œæ˜¾ç¤ºé€‰æ‹©å™¨
                            if len(li_deform_files) > 1:
                                st.markdown("---")
                                st.markdown("**å…¶ä»– Li Deformation æ–‡ä»¶:**")
                                for i, lf in enumerate(li_deform_files[1:], 1):
                                    st.text(f"  {i}. {os.path.basename(lf['path'])} (isovalue: {lf.get('isovalue', 'N/A')})")
                    except Exception as e:
                        st.error(f"Li Deformation å¯è§†åŒ–å¤±è´¥: {e}")

        elif structure_match:
            db_path = structure_match.group(1).strip()
            if os.path.exists(db_path):
                st.markdown("### ğŸ“Š ç»“æ„é¢„è§ˆ")
                
                with st.expander("ğŸ”¬ ç‚¹å‡»æŸ¥çœ‹ 3D åˆ†å­ç»“æ„ (å¯äº¤äº’)", expanded=True):
                    try:
                        atoms = load_structure_from_db(db_path)
                        if atoms:
                            viewer_html = create_gaussian_view_style_viewer(
                                atoms,
                                width=650,
                                height=500,
                                style="sphere+stick",
                                add_lighting=True
                            )
                            components.html(viewer_html, height=550, scrolling=False)
                            
                            st.caption(f"ğŸ“ ç»“æ„è·¯å¾„: `{db_path}`")
                            st.info("ğŸ’¡ æç¤ºï¼šæ‚¨å¯ä»¥è¯´ã€Œå¯¹ä¸Šé¢ç”Ÿæˆçš„ç»“æ„è¿›è¡Œç”µå­ç»“æ„åˆ†æã€æ¥ç»§ç»­åˆ†æ")
                        else:
                            st.warning("æ— æ³•åŠ è½½ç»“æ„é¢„è§ˆ")
                    except Exception as e:
                        st.error(f"ç»“æ„é¢„è§ˆå¤±è´¥: {e}")

        if download_match:
            file_path = download_match.group(1).strip()
            if os.path.exists(file_path):
                with open(file_path, "rb") as f:
                    st.download_button(
                        label="ğŸ“¦ ä¸‹è½½åˆ†æç»“æœå‹ç¼©åŒ… (.zip)",
                        data=f,
                        file_name=os.path.basename(file_path),
                        mime="application/zip",
                        key=f"{key_prefix}_download",
                    )


def main():
    """ä¸»å‡½æ•°ã€‚"""

    schedule_cleanup()
    cookie_manager = get_manager()

    if "user" not in st.session_state:
        st.session_state["user"] = None

    if st.session_state["user"] is None and not st.session_state.get("logout_flag", False):
        token = cookie_manager.get("auth_token")
        if token:
            user_info = db.verify_jwt_token(token)
            if user_info:
                st.session_state["user"] = user_info
                st.session_state["current_chat_id"] = None

    if st.session_state["user"] is None:
        login_ui(cookie_manager)
        return

    current_user = st.session_state["user"]
    if "suppress_autocreate" not in st.session_state:
        st.session_state["suppress_autocreate"] = False

    # 1. Sidebar
    with st.sidebar:
        st.write(f"ğŸ‘¤ **{current_user['username']}**")
        if st.button("ç™»å‡º", type="secondary"):
            st.session_state["user"] = None
            st.session_state["messages"] = []
            st.session_state["current_chat_id"] = None
            st.session_state["logout_flag"] = True
            cookie_manager.delete("auth_token")
            st.rerun()

        st.markdown("---")
        if st.button("+ æ–°å»ºå¯¹è¯", type="primary", use_container_width=True):
            st.session_state["suppress_autocreate"] = False
            new_id = db.create_conversation(current_user["id"], title="New Chat")
            st.session_state["current_chat_id"] = new_id
            st.session_state["messages"] = [{"role": "assistant", "content": WELCOME_MESSAGE}]
            db.add_message(new_id, "assistant", WELCOME_MESSAGE)
            st.rerun()

        st.markdown("### ğŸ•’ å†å²è®°å½•")
        conversations = db.get_user_conversations(current_user["id"])
        for chat in conversations:
            btn_type = "primary" if st.session_state.get("current_chat_id") == chat["id"] else "secondary"
            col1, col2 = st.columns([0.8, 0.2])
            with col1:
                if st.button(
                    f"ğŸ“„ {chat['title']}",
                    key=f"chat_{chat['id']}",
                    type=btn_type,
                    use_container_width=True,
                ):
                    st.session_state["current_chat_id"] = chat["id"]
                    msgs = db.get_conversation_messages(chat["id"])
                    st.session_state["messages"] = msgs if msgs else []
                    st.rerun()
            with col2:
                if st.button("ğŸ—‘ï¸", key=f"del_{chat['id']}"):
                    if len(conversations) == 1:
                        st.session_state["suppress_autocreate"] = True
                        st.session_state["current_chat_id"] = None
                        st.session_state["messages"] = []
                    safe_username = "".join([c for c in current_user["username"] if c.isalnum() or c in ("-", "_")])
                    chat_folder = os.path.join(get_project_root(), "users", safe_username, "output", str(chat["id"]))
                    if os.path.exists(chat_folder):
                        try:
                            shutil.rmtree(chat_folder)
                        except Exception as e:
                            st.toast(f"âš ï¸ æ–‡ä»¶å¤¹åˆ é™¤å¤±è´¥: {e}")
                    db.delete_conversation(chat["id"])
                    if st.session_state.get("current_chat_id") == chat["id"]:
                        st.session_state["current_chat_id"] = None
                        st.session_state["messages"] = []
                    st.rerun()

        st.markdown("---")
        st.header("æ¨¡å‹è®¾ç½®")
        model_name = st.selectbox(
            "é€‰æ‹©æ¨¡å‹",
            ["gemini-3-flash-preview", "gemini-3-pro-preview", "gemini-2.5-pro"],
            index=0,
        )
        api_key = os.getenv("GOOGLE_API_KEY", "")
        temperature = st.slider("Temperature", 0.0, 1.0, 0.0)

        st.markdown("---")
        st.header("ğŸ“š çŸ¥è¯†åº“ç®¡ç†")
        
        try:
            kb_stats = get_index_stats(api_key)
            if "error" not in kb_stats:
                st.metric("å·²ç´¢å¼•æ–‡æ¡£å—", kb_stats.get("total_documents", 0))
                st.caption(f"å·²ç´¢å¼•æ–‡ä»¶æ•°: {kb_stats.get('indexed_files', 0)}")
            else:
                st.warning("çŸ¥è¯†åº“æœªåˆå§‹åŒ–")
        except Exception:
            st.warning("çŸ¥è¯†åº“æœªåˆå§‹åŒ–")
        
        if current_user.get("username") in ADMIN_USERS:
            col_idx1, col_idx2 = st.columns(2)
            with col_idx1:
                if st.button("ğŸ”„ å¢é‡æ›´æ–°", use_container_width=True):
                    with st.spinner("æ­£åœ¨æ›´æ–°çŸ¥è¯†åº“ç´¢å¼•..."):
                        try:
                            stats = build_index(api_key, force_rebuild=False)
                            st.success(
                                f"ç´¢å¼•å®Œæˆï¼\n"
                                f"æ–°å¢: {stats['new_indexed']}, "
                                f"è·³è¿‡: {stats['skipped']}, "
                                f"å¤±è´¥: {stats['failed']}"
                            )
                        except Exception as e:
                            st.error(f"ç´¢å¼•å¤±è´¥: {e}")
            
            with col_idx2:
                if st.button("ğŸ”¨ é‡å»ºç´¢å¼•", use_container_width=True):
                    with st.spinner("æ­£åœ¨é‡å»ºçŸ¥è¯†åº“ç´¢å¼•ï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰..."):
                        try:
                            stats = build_index(api_key, force_rebuild=True)
                            st.success(
                                f"é‡å»ºå®Œæˆï¼\n"
                                f"å…±ç´¢å¼• {stats['new_indexed']} ä¸ªæ–‡ä»¶, "
                                f"{stats['total_chunks']} ä¸ªæ–‡æ¡£å—"
                            )
                        except Exception as e:
                            st.error(f"ç´¢å¼•å¤±è´¥: {e}")

    # 2. Session Init
    if st.session_state.get("current_chat_id") is None:
        user_conversations = db.get_user_conversations(current_user["id"])
        if user_conversations:
            latest_chat = user_conversations[0]
            st.session_state["current_chat_id"] = latest_chat["id"]
            msgs = db.get_conversation_messages(latest_chat["id"])
            st.session_state["messages"] = msgs if msgs else []
        else:
            if st.session_state.get("suppress_autocreate"):
                st.session_state["current_chat_id"] = None
                st.session_state["messages"] = []
            else:
                new_id = db.create_conversation(current_user["id"], title="New Chat")
                st.session_state["current_chat_id"] = new_id
                st.session_state["messages"] = [{"role": "assistant", "content": WELCOME_MESSAGE}]
                db.add_message(new_id, "assistant", WELCOME_MESSAGE)

    if st.session_state.get("current_chat_id") is None:
        st.title("ğŸ§ª EMolAgent")
        st.info("æš‚æ— å¯¹è¯ï¼Œè¯·åœ¨å·¦ä¾§ç‚¹å‡» [+ æ–°å»ºå¯¹è¯] æŒ‰é’®ã€‚")
        return

    # 3. LLM Setup
    if not api_key:
        st.warning("âš ï¸ Google API Key æ— æ•ˆã€‚")
        st.stop()

    try:
        agent = build_agent(model_name=model_name, temperature=temperature, api_key=api_key)
    except Exception as e:
        st.error(f"æ¨¡å‹/Agent åˆå§‹åŒ–å¤±è´¥: {e}")
        st.stop()

    # 4. Chat Interface
    st.title("ğŸ§ª EMolAgent")

    if "messages" not in st.session_state:
        st.session_state["messages"] = []

    for idx, msg in enumerate(st.session_state["messages"]):
        render_message_with_download(
            role=msg["role"],
            content=msg["content"],
            key_prefix=f"history_{idx}",
        )

    # 5. Handle Input
    if prompt_input := st.chat_input("è¯·è¾“å…¥æŒ‡ä»¤..."):
        st.session_state.messages.append({"role": "user", "content": prompt_input})
        st.chat_message("user").write(prompt_input)

        current_chat_id = st.session_state["current_chat_id"]
        db.add_message(current_chat_id, "user", prompt_input)

        if len(st.session_state.messages) <= 2:
            db.update_conversation_title(current_chat_id, prompt_input[:20])

        config = {"configurable": {"thread_id": str(current_chat_id)}}
        context = Context(
            user_id=str(current_user.get("id")) if current_user else None,
            username=current_user.get("username") if current_user else None,
            chat_id=str(current_chat_id),
        )

        with st.spinner("æ­£åœ¨æ€è€ƒå’Œæ‰§è¡Œä»»åŠ¡..."):
            try:
                response: dict[str, Any] = agent.invoke(
                    {"messages": [{"role": "user", "content": prompt_input}]},
                    config=config,
                    context=context,
                )

                output_text = None
                msgs = response.get("messages") if isinstance(response, dict) else None
                if msgs and isinstance(msgs, list):
                    last = msgs[-1]
                    if isinstance(last, dict):
                        output_text = last.get("content")
                    else:
                        output_text = getattr(last, "content", None)

                if not output_text and isinstance(response, dict):
                    output_text = response.get("output") or response.get("structured_response")

                if not output_text:
                    output_text = str(response)

                output_text_str = normalize_chat_content(output_text)

                render_message_with_download(
                    role="assistant",
                    content=output_text_str,
                    key_prefix="current_run",
                )

                st.session_state.messages.append({"role": "assistant", "content": output_text_str})
                db.add_message(current_chat_id, "assistant", output_text_str)

            except Exception as e:
                error_msg = f"æ‰§è¡Œå‡ºé”™: {str(e)}"
                render_message_with_download(
                    role="assistant",
                    content=error_msg,
                    key_prefix="current_run_error",
                )
                st.session_state.messages.append({"role": "assistant", "content": error_msg})
                db.add_message(current_chat_id, "assistant", error_msg)


if __name__ == "__main__":
    main()
