import os
import json
# å¼ºåˆ¶è®© Python åœ¨è®¿é—®æœ¬åœ°æœåŠ¡æ—¶ä¸ä½¿ç”¨ä»£ç†
os.environ["NO_PROXY"] = "localhost,127.0.0.1"
os.environ.pop("HTTP_PROXY", None)
os.environ.pop("HTTPS_PROXY", None)

import streamlit as st
from langchain_community.chat_models import ChatOllama
from langchain.agents import initialize_agent, AgentType
from langchain.tools import StructuredTool
from langchain.callbacks import StreamlitCallbackHandler
from langchain.callbacks.base import BaseCallbackHandler

# å¯¼å…¥ä½ çš„å·¥å…·åº“
from tools_lib import run_dptb_inference, update_db_metadata, generate_viz_report

# --- 1. é¡µé¢é…ç½® ---
st.set_page_config(page_title="EMol-Vis Local Agent", page_icon="ğŸ§ª", layout="wide")
st.title("ğŸ§ª EMolAgent (Powered by Ollama)")

# --- ä¾§è¾¹æ ä¸æ§åˆ¶ ---
with st.sidebar:
    st.header("æ§åˆ¶é¢æ¿")
    
    # é‡ç½®æŒ‰é’®
    if st.button("ğŸ”„ é‡ç½®ä¼šè¯ / åœæ­¢æ–°ä»»åŠ¡", type="primary", use_container_width=True):
        st.session_state["messages"] = [
            {"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„æœ¬åœ° AI åŠ©æ‰‹ã€‚å…¨è‡ªåŠ¨æ¨¡å¼å·²å¯åŠ¨ï¼Œéšæ—¶å¾…å‘½ï¼"}
        ]
        st.rerun()

    st.markdown("---")
    st.header("æ¨¡å‹è®¾ç½®")
    model_name = st.selectbox("é€‰æ‹©æ¨¡å‹", ["llama3.1", "qwen2.5:7b", "mistral"], index=0)
    temperature = st.slider("æ¸©åº¦ (Temperature)", 0.0, 1.0, 0.0)

# --- 2. åˆå§‹åŒ–æœ¬åœ° LLM ---
llm = ChatOllama(
    model=model_name,
    temperature=temperature,
    base_url="http://localhost:11434"
)

# --- 3. å®šä¹‰å¢å¼ºç‰ˆå·¥å…· (è§£å†³â€œä¸çŸ¥é“ç»“æœâ€çš„é—®é¢˜) ---

def validate_path_exists(path: str, description: str):
    """æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™ç»ˆæ­¢"""
    if not path or not os.path.exists(path):
        st.error(f"â›”ï¸ **é”™è¯¯ï¼šç»ˆæ­¢æ‰§è¡Œ**\n\næ‰¾ä¸åˆ°{description}ï¼š`{path}`\n\nè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚")
        st.stop()
    return True

def run_dptb_inference_safe(data_root, model_path, output_dir="output", db_name="dump.db"):
    validate_path_exists(data_root, "æ•°æ®æ–‡ä»¶å¤¹")
    validate_path_exists(model_path, "æ¨¡å‹æ–‡ä»¶")
    return run_dptb_inference(data_root, model_path, output_dir, db_name)

def update_db_metadata_safe(input_db, input_paths_file, output_db="updated.db"):
    validate_path_exists(input_db, "è¾“å…¥æ•°æ®åº“")
    validate_path_exists(input_paths_file, "è·¯å¾„æ–‡ä»¶")
    return update_db_metadata(input_db, input_paths_file, output_db)

def generate_viz_report_smart(abs_ase_path, npy_folder_path):
    """
    å¢å¼ºç‰ˆï¼šç”ŸæˆæŠ¥å‘Šåï¼Œè‡ªåŠ¨è¯»å– json å†…å®¹è¿”å›ç»™ Agentã€‚
    è¿™æ · Agent å°±ä¸éœ€è¦â€œå†æ¬¡æŸ¥æ‰¾â€ï¼Œä¹Ÿä¸å®¹æ˜“äº§ç”Ÿå¹»è§‰ã€‚
    """
    validate_path_exists(abs_ase_path, "ASEæ•°æ®åº“")
    validate_path_exists(npy_folder_path, "NPYæ–‡ä»¶å¤¹")
    
    # 1. æ‰§è¡ŒåŸæœ‰çš„ç”Ÿæˆé€»è¾‘
    result_str = generate_viz_report(abs_ase_path, npy_folder_path)
    
    # 2. è‡ªåŠ¨å°è¯•è¯»å–ç”Ÿæˆçš„ test_results.json
    json_path = "test_results.json" # è¿™æ˜¯ tools_lib é‡Œå†™æ­»çš„è·¯å¾„
    json_content = ""
    
    if os.path.exists(json_path):
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                # å°† JSON è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œé™åˆ¶é•¿åº¦é˜²æ­¢ token æº¢å‡º
                json_content = json.dumps(data, indent=2, ensure_ascii=False)
        except Exception as e:
            json_content = f"(è¯»å–JSONå¤±è´¥: {str(e)})"
    else:
        json_content = "(æœªæ‰¾åˆ°ç”Ÿæˆçš„ test_results.json æ–‡ä»¶)"

    # 3. æ„é€ è¿”å›ç»™ Agent çš„ç»ˆæä¿¡æ¯
    final_observation = (
        f"{result_str}\n"
        f"--------------------------------------------------\n"
        f"ã€é‡è¦æç¤ºã€‘ä»¥ä¸‹æ˜¯ç”Ÿæˆçš„ JSON æŠ¥å‘Šå†…å®¹ï¼Œè¯·ç›´æ¥é€šè¿‡ Analysis æ€»ç»“æ­¤æ•°æ®ï¼Œå¹¶ç»™å‡ºç”Ÿæˆçš„ cube æ–‡ä»¶çš„è·¯å¾„, å‘Šè¯‰ç”¨æˆ·å†…å«html, å¯æŸ¥çœ‹å…·ä½“å›¾åƒç„¶åç»“æŸå¯¹è¯ï¼š\n"
        f"{json_content}"
    )
    return final_observation

tools = [
    StructuredTool.from_function(
        func=run_dptb_inference_safe,
        name="Run_Inference",
        description="Step 1. Run inference. Args: data_root, model_path."
    ),
    StructuredTool.from_function(
        func=update_db_metadata_safe,
        name="Update_Metadata",
        description="Step 2. Update metadata. Args: input_db, input_paths_file."
    ),
    StructuredTool.from_function(
        func=generate_viz_report_smart, # ä½¿ç”¨å¢å¼ºç‰ˆ
        name="Generate_Visualization",
        description="Step 3. Generate HTML report. Args: abs_ase_path, npy_folder_path."
    )
]

# --- 4. åˆå§‹åŒ– Agent ---

# å¼ºåŒ– Promptï¼Œé˜²æ­¢æ­»å¾ªç¯
custom_system_prefix = """
ä½ æ˜¯ä¸€ä¸ªè®¡ç®—åŒ–å­¦ AI åŠ©æ‰‹ã€‚è¯·æŒ‰é¡ºåºæ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š
1. Run_Inference
2. Update_Metadata
3. Generate_Visualization
4. è¯·æ ¹æ®è¿”å›çš„ JSON æ•°æ®å›ç­”ç”¨æˆ·çš„è¯¯å·®ç»“æœï¼Œå¹¶ç»™å‡ºç”Ÿæˆçš„ cube æ–‡ä»¶çš„è·¯å¾„ï¼Œå‘Šè¯‰ç”¨æˆ·å†…å« html æ–‡ä»¶, å¯æŸ¥çœ‹å…·ä½“å›¾åƒ, ç„¶åç»“æŸå¯¹è¯ã€‚
ã€æé‡è¦è§„åˆ™ã€‘ï¼š
- å½“ä½ æ‰§è¡Œå®Œ "Generate_Visualization" åï¼Œå·¥å…·ä¼šç›´æ¥è¿”å› JSON æ•°æ®å†…å®¹ã€‚
- **ä¸€æ—¦ä½ çœ‹åˆ°äº† JSON æ•°æ®ï¼Œå¿…é¡»ç«‹å³åœæ­¢è°ƒç”¨ä»»ä½•å·¥å…·ï¼**
- **ç»å¯¹ç¦æ­¢**å†æ¬¡è°ƒç”¨ Run_Inferenceã€‚
- è¯·ç›´æ¥æ ¹æ®è¿”å›çš„ JSON æ•°æ®å›ç­”ç”¨æˆ·çš„è¯¯å·®ç»“æœï¼Œå¹¶ç»™å‡ºç”Ÿæˆçš„ cube æ–‡ä»¶çš„è·¯å¾„, å‘Šè¯‰ç”¨æˆ·å†…å«html, å¯æŸ¥çœ‹å…·ä½“å›¾åƒ, ç„¶åç»“æŸå¯¹è¯ã€‚
- ä½ çš„æœ€åä¸€å¥å¿…é¡»æ˜¯ï¼š"ä»»åŠ¡å·²å®Œæˆã€‚"
"""

try:
    agent_executor = initialize_agent(
        tools=tools,
        llm=llm,
        agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,
        verbose=True,
        handle_parsing_errors=True,
        max_iterations=10, # ã€é˜²æ­¢æ­»å¾ªç¯ã€‘é™åˆ¶æœ€å¤§æ­¥éª¤æ•°ä¸º10
        early_stopping_method="generate",
        agent_kwargs={
            "prefix": custom_system_prefix,
        }
    )

except Exception as e:
    st.error(f"Agent åˆå§‹åŒ–å¤±è´¥: {repr(e)}")
    st.stop()

# --- 5. èŠå¤©é€»è¾‘ ---

if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {"role": "assistant", "content": "ä½ å¥½ï¼è¯·å‘Šè¯‰æˆ‘æ•°æ®è·¯å¾„ã€æ¨¡å‹è·¯å¾„å’Œè·¯å¾„æ–‡ä»¶ä½ç½®ã€‚"}
    ]

# æ˜¾ç¤ºå†å²æ¶ˆæ¯
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# å¤„ç†è¾“å…¥
if prompt_input := st.chat_input("è¯·è¾“å…¥æŒ‡ä»¤..."):
    st.session_state.messages.append({"role": "user", "content": prompt_input})
    st.chat_message("user").write(prompt_input)

    with st.chat_message("assistant"):
        st_callback = StreamlitCallbackHandler(st.container())
        try:
            # ä½¿ç”¨ invoke æ¥å£ï¼ˆLangChain æ–°ç‰ˆæ¨èï¼‰
            response = agent_executor.run(
                prompt_input, 
                callbacks=[st_callback]
            )
            st.write(response) # æ˜¾ç¤ºæœ€ç»ˆå›ç­”
            st.session_state.messages.append({"role": "assistant", "content": response})
        except Exception as e:
            # æ•è·å¯èƒ½çš„é”™è¯¯ï¼ˆå¦‚è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼‰
            error_msg = f"æ‰§è¡Œä¸­æ–­æˆ–å‡ºé”™: {str(e)}"
            if "Agent stopped due to iteration limit" in str(e):
                error_msg = "âš ï¸ ä»»åŠ¡å› æ­¥éª¤è¿‡å¤šå·²å¼ºåˆ¶åœæ­¢ï¼ˆé˜²æ­¢æ­»å¾ªç¯ï¼‰ã€‚è¯·æ£€æŸ¥ä¸Šæ–¹æ—¥å¿—çœ‹æ˜¯å¦å·²å®Œæˆå…³é”®æ­¥éª¤ã€‚"
            st.error(error_msg)
            st.session_state.messages.append({"role": "assistant", "content": error_msg})