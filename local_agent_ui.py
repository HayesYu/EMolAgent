import os
# å¼ºåˆ¶è®© Python åœ¨è®¿é—®æœ¬åœ°æœåŠ¡æ—¶ä¸ä½¿ç”¨ä»£ç†
os.environ["NO_PROXY"] = "localhost,127.0.0.1"
# ä¸ºäº†ä¿é™©ï¼Œä¹Ÿå¯ä»¥æŠŠä¸‹é¢è¿™ä¸¤è¡ŒåŠ ä¸Šï¼Œå½»åº•å±è”½è„šæœ¬çš„ä»£ç†è®¾ç½®
os.environ.pop("HTTP_PROXY", None)
os.environ.pop("HTTPS_PROXY", None)
import streamlit as st
from langchain_community.chat_models import ChatOllama
# [å…³é”®ä¿®æ”¹] æ”¹å›ä½¿ç”¨ initialize_agentï¼Œå®ƒèƒ½è‡ªåŠ¨å¤„ç† Prompt ç»“æ„ï¼Œä¸å†æ‰‹åŠ¨æ‹¼æ¥
from langchain.agents import initialize_agent, AgentType
from langchain.tools import StructuredTool
from langchain.callbacks import StreamlitCallbackHandler

# å¯¼å…¥ä½ çš„å·¥å…·åº“
from tools_lib import run_dptb_inference, update_db_metadata, generate_viz_report

# --- 1. é¡µé¢é…ç½® ---
st.set_page_config(page_title="EMol-Vis Local Agent", page_icon="ğŸ§ª")
st.title("ğŸ§ª EMolAgent (Powered by Ollama)")

with st.sidebar:
    st.header("æ¨¡å‹è®¾ç½®")
    model_name = st.selectbox("é€‰æ‹©æ¨¡å‹", ["llama3.1", "qwen2.5:7b", "mistral"], index=0)
    temperature = st.slider("æ¸©åº¦ (Temperature)", 0.0, 1.0, 0.0)
    st.markdown("---")
    st.markdown("âœ… **çŠ¶æ€**: æœ¬åœ°è¿è¡Œä¸­ (Classic Mode)")
    st.markdown("ğŸš« **ç½‘ç»œ**: æœ¬åœ°")

# --- 2. åˆå§‹åŒ–æœ¬åœ° LLM ---
llm = ChatOllama(
    model=model_name,
    temperature=temperature,
    base_url="http://localhost:11434"
)

# --- 3. å®šä¹‰å·¥å…· ---
tools = [
    StructuredTool.from_function(
        func=run_dptb_inference,
        name="Run_Inference",
        description="Step 1. Run deep learning inference. Args: data_root, model_path."
    ),
    StructuredTool.from_function(
        func=update_db_metadata,
        name="Update_Metadata",
        description="Step 2. Correct spin/charge metadata. Args: input_db, input_paths_file."
    ),
    StructuredTool.from_function(
        func=generate_viz_report,
        name="Generate_Visualization",
        description="Step 3. Generate HTML/MAE report. Args: abs_ase_path, npy_folder_path."
    )
]

# --- 4. åˆå§‹åŒ– Agent (ä½¿ç”¨ initialize_agent) ---

# å®šä¹‰ä½ çš„ä¸ªæ€§åŒ–ç³»ç»Ÿæç¤ºè¯ (System Prompt)
# æˆ‘ä»¬æŠŠå®ƒæ”¾åœ¨ Agent çš„ "prefix" ä¸­ï¼Œè¿™æ ·æ—¢ä¿ç•™äº†ä½ çš„è¦æ±‚ï¼Œåˆä¸ä¼šç ´å Agent çš„å†…éƒ¨ç»“æ„
custom_system_prefix = """
ä½ æ˜¯ä¸€ä¸ªç²¾é€š Python å’Œè®¡ç®—åŒ–å­¦çš„ AI åŠ©æ‰‹ã€‚
ä½ çš„ç›®æ ‡æ˜¯å¸®åŠ©ç”¨æˆ·å®Œæˆ DeepPTB æ¨¡å‹çš„æ¨ç†å’Œåˆ†ææµç¨‹ã€‚

è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹æ ¸å¿ƒè§„åˆ™ï¼š
1. **ä¸¥æ ¼é¡ºåº**ï¼šå¿…é¡»æŒ‰ç…§ Run_Inference -> Update_Metadata -> Generate_Visualization çš„é¡ºåºæ‰§è¡Œã€‚
2. **å‚æ•°æ£€æŸ¥**ï¼šå¦‚æœå·¥å…·æŠ¥é”™ï¼Œè¯·ä»”ç»†æ£€æŸ¥é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æ–‡ä»¶è·¯å¾„ä¸å­˜åœ¨ï¼‰å¹¶å°è¯•ä¿®å¤å‚æ•°åé‡è¯•ã€‚
3. **æœ€ç»ˆåé¦ˆ**ï¼šåœ¨å¯è§†åŒ–ç”Ÿæˆåï¼Œæ˜ç¡®å‘Šè¯‰ç”¨æˆ· HTML æ–‡ä»¶çš„ä¿å­˜è·¯å¾„ã€‚
"""

try:
    # [æ ¸å¿ƒä¿®æ”¹] ä½¿ç”¨ initialize_agent è‡ªåŠ¨ç»„è£…
    # agent_kwargs ç”¨äºæ³¨å…¥ä½ çš„è‡ªå®šä¹‰ Prompt
    agent_executor = initialize_agent(
        tools=tools,
        llm=llm,
        agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,
        verbose=True,
        handle_parsing_errors=True, # è‡ªåŠ¨çº æ­£æ ¼å¼é”™è¯¯
        agent_kwargs={
            "prefix": custom_system_prefix, # æ³¨å…¥ä½ çš„æŒ‡ä»¤
            # "input_variables": ["input", "agent_scratchpad"] # è®©å®ƒè‡ªåŠ¨å¤„ç†
        }
    )

except Exception as e:
    # ä½¿ç”¨ repr(e) æ‰“å°å®Œæ•´çš„é”™è¯¯å¯¹è±¡ï¼Œé˜²æ­¢é”™è¯¯ä¿¡æ¯ä¸ºç©º
    st.error(f"Agent åˆå§‹åŒ–å¤±è´¥: {repr(e)}")
    st.stop()

# --- 5. èŠå¤©ç•Œé¢é€»è¾‘ ---

if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„æœ¬åœ° AI åŠ©æ‰‹ã€‚å…¨è‡ªåŠ¨æ¨¡å¼å·²å¯åŠ¨ï¼Œéšæ—¶å¾…å‘½ï¼"}
    ]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

if prompt_input := st.chat_input("è¯·è¾“å…¥æŒ‡ä»¤..."):
    st.session_state.messages.append({"role": "user", "content": prompt_input})
    st.chat_message("user").write(prompt_input)

    with st.chat_message("assistant"):
        st_callback = StreamlitCallbackHandler(st.container())
        try:
            # initialize_agent è¿”å›çš„å°±æ˜¯ Executorï¼Œç›´æ¥è°ƒç”¨ run æˆ– invoke
            response = agent_executor.run(
                prompt_input, 
                callbacks=[st_callback]
            )
            st.write(response)
            st.session_state.messages.append({"role": "assistant", "content": response})
        except Exception as e:
            st.error(f"æ‰§è¡Œå‡ºé”™: {repr(e)}")
            st.session_state.messages.append({"role": "assistant", "content": f"æ‰§è¡Œå‡ºé”™: {e}"})