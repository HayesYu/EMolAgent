import os
import json

import streamlit as st
# --- 1. å¯¼å…¥å¿…è¦çš„åº“ ---
from langchain_google_genai import ChatGoogleGenerativeAI
# æ³¨æ„ï¼šè¿™é‡Œå¼•å…¥äº† create_tool_calling_agent å’Œ AgentExecutor
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.tools import StructuredTool
from langchain.callbacks import StreamlitCallbackHandler

from tools_lib import run_dptb_inference, update_db_metadata, generate_viz_report

# --- 2. ç¯å¢ƒå˜é‡é…ç½® ---
os.environ["NO_PROXY"] = "localhost,127.0.0.1,0.0.0.0"
os.environ["HTTP_PROXY"] = "http://127.0.0.1:7890"
os.environ["HTTPS_PROXY"] = "http://127.0.0.1:7890"

DEFAULT_MODEL_PATH = "/home/hayes/EMolAgent_demo/nnenv.iter147201.pth"

# --- 3. é¡µé¢é…ç½® ---
st.set_page_config(page_title="EMol-Vis Local Agent", page_icon="ğŸ§ª", layout="wide")
st.title("ğŸ§ª EMolAgent")

# --- 4. ä¾§è¾¹æ ä¸æ§åˆ¶ ---
with st.sidebar:
    st.header("æ§åˆ¶é¢æ¿")
    
    # é‡ç½®æŒ‰é’®
    if st.button("ğŸ”„ é‡ç½®ä¼šè¯ / åœæ­¢æ–°ä»»åŠ¡", type="primary", use_container_width=True):
        st.session_state["messages"] = [
            {"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„ AI åŠ©æ‰‹ã€‚å…¨è‡ªåŠ¨æ¨¡å¼å·²å¯åŠ¨ï¼Œéšæ—¶å¾…å‘½ï¼"}
        ]
        st.rerun()

    st.markdown("---")
    st.header("æ¨¡å‹è®¾ç½®")
    model_name = st.selectbox(
        "é€‰æ‹©æ¨¡å‹", 
        ["gemini-2.5-flash", "gemini-2.5-pro", "gemini-2.0-flash-exp"], 
        index=0
    )
    api_key = st.text_input("Google API Key", type="password", help="åœ¨æ­¤è¾“å…¥ä½ çš„ Gemini API Key")
    temperature = st.slider("æ¸©åº¦ (Temperature)", 0.0, 1.0, 0.0)
    st.info(f"**å½“å‰é»˜è®¤æ¨¡å‹**:\n{os.path.basename(DEFAULT_MODEL_PATH)}")

# --- 5. åˆå§‹åŒ–æœ¬åœ° LLM ---
if not api_key:
    st.warning("âš ï¸ è¯·åœ¨å·¦ä¾§ä¾§è¾¹æ è¾“å…¥ Google API Key ä»¥å¯åŠ¨ Agentã€‚")
    st.stop()

try:
    llm = ChatGoogleGenerativeAI(
        model=model_name,
        google_api_key=api_key,
        temperature=temperature,
        convert_system_message_to_human=True, 
    )
except Exception as e:
    st.error(f"æ¨¡å‹è¿æ¥å¤±è´¥: {e}")
    st.stop()

# --- 6. å®šä¹‰å¢å¼ºç‰ˆå·¥å…· ---

def validate_path_exists(path: str, description: str):
    """æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™ç»ˆæ­¢"""
    if not path or not os.path.exists(path):
        st.error(f"â›”ï¸ **é”™è¯¯ï¼šç»ˆæ­¢æ‰§è¡Œ**\n\næ‰¾ä¸åˆ°{description}ï¼š`{path}`\n\nè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚")
        st.stop()
    return True

def run_dptb_inference_safe(data_root, model_path=None, output_dir="output", db_name="dump.db"):
    if model_path in ["None", "null", "", None]:
        model_path = DEFAULT_MODEL_PATH
        st.toast(f"â„¹ï¸ å·²è‡ªåŠ¨åŠ è½½é»˜è®¤æ¨¡å‹: {os.path.basename(model_path)}")
    validate_path_exists(data_root, "æ•°æ®æ–‡ä»¶å¤¹")
    validate_path_exists(model_path, "æ¨¡å‹æ–‡ä»¶")
    return run_dptb_inference(data_root, model_path, output_dir, db_name)

def update_db_metadata_safe(input_db, input_paths_file, output_db="updated.db"):
    validate_path_exists(input_db, "è¾“å…¥æ•°æ®åº“")
    validate_path_exists(input_paths_file, "è·¯å¾„æ–‡ä»¶")
    return update_db_metadata(input_db, input_paths_file, output_db)

def generate_viz_report_smart(abs_ase_path, npy_folder_path):
    validate_path_exists(abs_ase_path, "ASEæ•°æ®åº“")
    validate_path_exists(npy_folder_path, "NPYæ–‡ä»¶å¤¹")
    
    result_str = generate_viz_report(abs_ase_path, npy_folder_path)
    json_path = "test_results.json" 
    json_content = ""
    
    if os.path.exists(json_path):
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                json_content = json.dumps(data, indent=2, ensure_ascii=False)
        except Exception as e:
            json_content = f"(è¯»å–JSONå¤±è´¥: {str(e)})"
    else:
        json_content = "(æœªæ‰¾åˆ°ç”Ÿæˆçš„ test_results.json æ–‡ä»¶)"

    final_observation = (
        f"{result_str}\n"
        f"--------------------------------------------------\n"
        f"ã€é‡è¦æç¤ºã€‘ä»¥ä¸‹æ˜¯ç”Ÿæˆçš„ JSON æŠ¥å‘Šå†…å®¹ï¼Œè¯·ç›´æ¥é€šè¿‡ Analysis æ€»ç»“æ­¤æ•°æ®ï¼Œå¹¶ç»™å‡ºç”Ÿæˆçš„ cube æ–‡ä»¶çš„è·¯å¾„, å‘Šè¯‰ç”¨æˆ·å†…å«html, å¯æŸ¥çœ‹å…·ä½“å›¾åƒç„¶åç»“æŸå¯¹è¯ï¼š\n"
        f"{json_content}"
    )
    return final_observation

tools = [
    StructuredTool.from_function(
        func=run_dptb_inference_safe,
        name="Run_Inference",
        description="Step 1. Run inference. Args: data_root. (Model path defaults to built-in if not provided)"
    ),
    StructuredTool.from_function(
        func=update_db_metadata_safe,
        name="Update_Metadata",
        description="Step 2. Update metadata. Args: input_db, input_paths_file."
    ),
    StructuredTool.from_function(
        func=generate_viz_report_smart,
        name="Generate_Visualization",
        description="Step 3. Generate HTML report. Args: abs_ase_path, npy_folder_path."
    )
]

# --- 7. åˆå§‹åŒ– Agent (ä½¿ç”¨æ–°ç‰ˆ Tool Calling API) ---

custom_system_prefix = """
ä½ æ˜¯ä¸€ä¸ªè®¡ç®—åŒ–å­¦ AI åŠ©æ‰‹ã€‚è¯·æŒ‰é¡ºåºæ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š
1. Run_Inference
2. Update_Metadata
3. Generate_Visualization
4. è¯·æ ¹æ®è¿”å›çš„ JSON æ•°æ®å›ç­”ç”¨æˆ·çš„è¯¯å·®ç»“æœï¼Œå¹¶ç»™å‡ºç”Ÿæˆçš„ cube æ–‡ä»¶çš„è·¯å¾„ï¼Œå‘Šè¯‰ç”¨æˆ·å†…å« html æ–‡ä»¶, å¯æŸ¥çœ‹å…·ä½“å›¾åƒ, ç„¶åç»“æŸå¯¹è¯ã€‚
ã€æé‡è¦è§„åˆ™ã€‘ï¼š
- å½“ä½ æ‰§è¡Œå®Œ "Generate_Visualization" åï¼Œå·¥å…·ä¼šç›´æ¥è¿”å› JSON æ•°æ®å†…å®¹ã€‚
- **ä¸€æ—¦ä½ çœ‹åˆ°äº† JSON æ•°æ®ï¼Œå¿…é¡»ç«‹å³åœæ­¢è°ƒç”¨ä»»ä½•å·¥å…·ï¼**
- **ç»å¯¹ç¦æ­¢**å†æ¬¡è°ƒç”¨ Run_Inferenceã€‚
- è¯·ç›´æ¥æ ¹æ®è¿”å›çš„ JSON æ•°æ®å›ç­”ç”¨æˆ·çš„è¯¯å·®ç»“æœï¼Œå¹¶ç»™å‡ºç”Ÿæˆçš„ cube æ–‡ä»¶çš„è·¯å¾„, å‘Šè¯‰ç”¨æˆ·å†…å«html, å¯æŸ¥çœ‹å…·ä½“å›¾åƒ, ç„¶åç»“æŸå¯¹è¯ã€‚
- ä½ çš„æœ€åä¸€å¥å¿…é¡»æ˜¯ï¼š"ä»»åŠ¡å·²å®Œæˆã€‚"
"""

try:
    # 1. åˆ›å»º Prompt (æ˜¾å¼åŒ…å« system å’Œ placeholder)
    prompt = ChatPromptTemplate.from_messages([
        ("system", custom_system_prefix),
        ("human", "{input}"),
        MessagesPlaceholder(variable_name="agent_scratchpad"),
    ])

    # 2. åˆ›å»º Agent (é’ˆå¯¹ Gemini/OpenAI ç­‰æ”¯æŒ Function Calling çš„æ¨¡å‹)
    agent = create_tool_calling_agent(llm, tools, prompt)

    # 3. åˆ›å»ºæ‰§è¡Œå™¨
    agent_executor = AgentExecutor(
        agent=agent,
        tools=tools,
        verbose=True,
        max_iterations=10, 
    )

except Exception as e:
    st.error(f"Agent åˆå§‹åŒ–å¤±è´¥: {repr(e)}")
    st.stop()

# --- 8. èŠå¤©é€»è¾‘ ---

if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {"role": "assistant", "content": "ä½ å¥½ï¼è¯·å‘Šè¯‰æˆ‘æ•°æ®è·¯å¾„ã€å’Œ Spin/Charge æ˜ å°„æ–‡ä»¶ä½ç½®ã€‚"}
    ]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

if prompt_input := st.chat_input("è¯·è¾“å…¥æŒ‡ä»¤..."):
    st.session_state.messages.append({"role": "user", "content": prompt_input})
    st.chat_message("user").write(prompt_input)

    with st.chat_message("assistant"):
        st_callback = StreamlitCallbackHandler(st.container())
        try:
            # --- å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨ .invoke è€Œä¸æ˜¯ .run ---
            response = agent_executor.invoke(
                {"input": prompt_input}, 
                config={"callbacks": [st_callback]}
            )
            # invoke è¿”å›çš„æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œç»“æœåœ¨ 'output' é”®ä¸­
            output_text = response["output"]
            
            st.write(output_text) 
            st.session_state.messages.append({"role": "assistant", "content": output_text})
        except Exception as e:
            error_msg = f"æ‰§è¡Œä¸­æ–­æˆ–å‡ºé”™: {str(e)}"
            if "Agent stopped due to iteration limit" in str(e):
                error_msg = "âš ï¸ ä»»åŠ¡å› æ­¥éª¤è¿‡å¤šå·²å¼ºåˆ¶åœæ­¢ï¼ˆé˜²æ­¢æ­»å¾ªç¯ï¼‰ã€‚"
            st.error(error_msg)
            st.session_state.messages.append({"role": "assistant", "content": error_msg})